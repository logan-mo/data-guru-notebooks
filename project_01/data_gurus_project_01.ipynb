{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee4fa018",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2063d6ca-7214-4723-9791-a0e0da72b822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from influxdb.resultset import ResultSet\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from influxdb import InfluxDBClient\n",
    "from pandas import json_normalize\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08cf0597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "LOG_FORMAT = \"%(name)s: %(asctime)s - %(message)s\"\n",
    "logging.basicConfig(level = logging.INFO,\n",
    "                    filename = 'output.log', \n",
    "                    format= LOG_FORMAT,\n",
    "                    filemode = 'a')\n",
    "logger = logging.getLogger('Data_Query')\n",
    "\n",
    "# Disabling loggers from influxdb and other modules to prevernt them from pollution log files\n",
    "logger_influxDB = logging.getLogger('py4j.clientserver')\n",
    "logger_root = logging.getLogger('root')\n",
    "logger_influxDB.disabled = True\n",
    "logger_root.disabled = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5c7749",
   "metadata": {},
   "source": [
    "# Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e992c1f-a722-44cc-b3b5-61155d7e42f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Joining Data').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feca6cd1",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1afcdf1-0e4d-4e80-b4ce-6866116285bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data, removing duplicates, joining tables and selecting desired columns\n",
    "\n",
    "df2  = spark.read.option('header', 'true').csv('tsdb_data.csv')                         # Reading data from csv file\n",
    "logger.info(\"Successfully Read Data from tsdb_data.csv file\")\n",
    "temp = df2.na.drop(subset=[\"anlage_id\"])                                                # Removing rows with null values in anlage_id column\n",
    "temp = temp.groupBy('anlage_id','steuereinheitnummer').count().drop('count')            # Removing duplicates using groupby \n",
    "temp = temp.dropDuplicates()                                                            # Removing duplicates using dropDuplicates (Just in case)\n",
    "\n",
    "df1 = spark.read.option('header', 'true').csv('a_inst_all.csv')                         # Reading data from csv file\n",
    "logger.info(\"Successfully Read Data from a_inst_all.csv file\")\n",
    "df3 = df1.join(temp,df1.Steuereinheit_Nr  ==  temp.steuereinheitnummer,\"inner\")         # Joining tables using Control Unit Number\n",
    "df3 = df3.dropDuplicates()                                                              # Removing duplicates\n",
    "df4 = df3.select('anlage_id','steuereinheitnummer')                                     # Selecting desired columns\n",
    "\n",
    "#df4.show()\n",
    "logger.info(\"Successful Data Preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08e9c30b-6f8c-4899-aa48-ae93354340b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the data from spark table to a dataframe so that we can get a list of ids for easy access\n",
    "new_data = df4.toPandas()\n",
    "device_ids = list(new_data['steuereinheitnummer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c0cd17",
   "metadata": {},
   "source": [
    "# Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90b2af71-03b0-4f8c-ac1c-eea32fca5297",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFLUX_DB_USER      = \"****************\"\n",
    "INFLUX_DB_PASSWORD  = \"****************\"\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08083b32",
   "metadata": {},
   "source": [
    "# Data Fetching and Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88987e74-6404-48d8-81e5-26b39c7e8df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list of servers to connect to\n",
    "serverlist = [\n",
    "    \"influxdb****************.com\",\n",
    "    \"influxdb****************.com\",\n",
    "    \"influxdb****************.com\",\n",
    "    \"influxdb****************.com\",\n",
    "    \"influxdb****************.com\",\n",
    "    \"influxdb****************.com\"\n",
    "]\n",
    "\n",
    "max_ids     = 500                                                       # Set to -1 to run over the whole data\n",
    "batch_size  = 50                                                        # Number of ids to query the servers at once\n",
    "id_list = device_ids[:max_ids] if max_ids != -1 else device_ids         # Extract the ids that are being queried to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e61b08f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function takes in a list ofqueries the database for the data, parses the response and returns a list of dictionaries containing the data\n",
    "\"\"\"\n",
    "def query_influxdb(query):\n",
    "    query_response = []\n",
    "    res_count = 0   \n",
    "    for influxdb in serverlist:\n",
    "        client = InfluxDBClient(\n",
    "            host=influxdb,\n",
    "            port=443,\n",
    "            database=\"tsdb\",\n",
    "            username=INFLUX_DB_USER,\n",
    "            password=INFLUX_DB_PASSWORD,\n",
    "            ssl=True,\n",
    "            verify_ssl=False,\n",
    "            pool_size=1\n",
    "        )\n",
    "        data = client.query(query)\n",
    "        for row in data.items():\n",
    "            # Parse the date recieved from the database\n",
    "            query_response.extend([dict(**{'mainControllerSerialDeviceId':row[0][1][\"mainControllerSerialDeviceId\"], \n",
    "                            'time': datetime.strptime(item['time'], '%Y-%m-%dT%H:%M:%SZ')}) \n",
    "                            for item in list(row[1])])\n",
    "        res_count += len(data)\n",
    "    print(f\"Found {res_count} result(s)\")\n",
    "    return query_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bb75bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating all the queries to be sent to the database\n",
    "queries = []\n",
    "for idx in range(0, len(device_ids), batch_size):\n",
    "    if max_ids != -1 and idx == max_ids:\n",
    "        break\n",
    "                 \n",
    "    start = idx\n",
    "    end = idx + batch_size\n",
    "    if end > len(device_ids):\n",
    "        end = len(device_ids)\n",
    "    \n",
    "    ids = \"|\".join([x for x in device_ids[start:end]])                  # Join the ids to a string for the query in format \"id1|id2|id3|...\"\n",
    "    \n",
    "    query = f\"\"\"SELECT * FROM \"wallbox_v123_measurement\" WHERE mainControllerSerialDeviceId =~ /{ids}/ GROUP BY mainControllerSerialDeviceId ORDER BY time ASC LIMIT 1\"\"\"\n",
    "    queries.append(query)\n",
    "logger.info(\"Successfully prepared Database queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f06bc55-b6c9-4eaf-b20c-411525a07dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f8856c",
   "metadata": {},
   "source": [
    "## Multiprocessed data fetching from the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de49e0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 result(s)\n",
      "Found 2 result(s)\n",
      "Found 5 result(s)\n",
      "Found 6 result(s)\n",
      "Found 3 result(s)\n",
      "Found 1 result(s)\n",
      "Found 0 result(s)\n",
      "Found 2 result(s)\n",
      "Found 6 result(s)\n",
      "Found 6 result(s)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Run the queries in parallel\n",
    "start_time = time.time()\n",
    "with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "    query_responses = pool.map(query_influxdb, queries)\n",
    "#print(\"Time taken: %s seconds\" % (time.time() - start_time))\n",
    "logger.info(f\"Successfully retrieved data from database in {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28e8e483-972e-4e7a-b82c-f9794005b544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(query_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8019cd8f",
   "metadata": {},
   "source": [
    "# Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d9377f3-de12-4743-8cc5-ea58b9128757",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_results = []\n",
    "for response in query_responses:\n",
    "    if len(response) > 0:\n",
    "        query_results.extend(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5638ba1-ffc4-4397-a2f1-591b0b52e871",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Total results from database: {len(query_results)}\")\n",
    "logger.info(f\"Total results from database: {len(query_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "982eb127-2b34-44a8-bc8e-96be877d15fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the query data into a dataframe. Sort them by Id and Time and keep the first.\n",
    "# This way only the latest entry corresponding to a specific ID is kept\n",
    "\n",
    "query_dataframe = pd.DataFrame(query_results) \\\n",
    "    .sort_values(by=['mainControllerSerialDeviceId', 'time']) \\\n",
    "    .drop_duplicates(subset=['mainControllerSerialDeviceId'], keep='first') \\\n",
    "    .reset_index(drop=True)\n",
    "logger.info(\"Retrieved latest record for every device id.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f7476d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f'Number of ids with a response: {len(query_dataframe)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "483d4056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38371912",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Explanation for the code below:\n",
    "\n",
    "assuming the id_list has contents : [a, b, c, d, e, f, g]\n",
    "and we get responses from databases for ids: [a_1, a_2, c_1, and d_1]\n",
    "the following code will make a dataframe in the format:\n",
    "\n",
    "pd.DataFrame({\n",
    "    'id': [a_1, a_2, b, c_1, d_1, e, f, g],\n",
    "    'time': [time_a_1, time_a_2, None, time_c_1, time_d_1, None, None, None]\n",
    "})\n",
    "'''\n",
    "\n",
    "\n",
    "idx = 0\n",
    "time_list = [None for x in id_list]\n",
    "\n",
    "while idx < len(query_dataframe):\n",
    "    id = query_dataframe.loc[idx, 'mainControllerSerialDeviceId']\n",
    "    \n",
    "    if id[:-2] in id_list:\n",
    "        # Get the index\n",
    "        id_idx = id_list.index(query_dataframe.iloc[idx]['mainControllerSerialDeviceId'][:-2])\n",
    "        \n",
    "        # Delete the entry at the index in both lists\n",
    "        del id_list[id_idx]\n",
    "        del time_list[id_idx]\n",
    "\n",
    "        # Put all query results with the same id in both lists\n",
    "        while query_dataframe.loc[idx, 'mainControllerSerialDeviceId'][:-2] == id[:-2]:\n",
    "            id_list.insert(id_idx, query_dataframe.loc[idx, 'mainControllerSerialDeviceId'])\n",
    "            time_list.insert(id_idx, query_dataframe.loc[idx, 'time'])\n",
    "            id_idx +=1 \n",
    "            idx += 1\n",
    "            if idx == len(query_dataframe):\n",
    "                break\n",
    "    else:\n",
    "        idx += 1\n",
    "\n",
    "output_dataframe = pd.DataFrame({\n",
    "    'id': id_list,\n",
    "    'time': time_list\n",
    "})\n",
    "\n",
    "logger.info(\"Database response converted into desired format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45dc4267-9a8e-4b8a-8764-b2f51bf50e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22854b2",
   "metadata": {},
   "source": [
    "# Saving the final table as a Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0bffb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table_path = \"delta_id_table.parquet\"\n",
    "try:\n",
    "    output_delta_table = spark.read.parquet(delta_table_path)\n",
    "    logger.info(\"Delta table already exists, writing to it in append mode.\")\n",
    "    output_delta_table.write.mode('append').parquet(delta_table_path)   \n",
    "except AnalysisException as e:\n",
    "    logger.info(f\"Delta table Does Not Exists, Creating a new Delta table with the name \\'{delta_table_path}\\'\")\n",
    "    output_delta_table = spark.createDataFrame(output_dataframe)\n",
    "    output_delta_table.write.format(\"parquet\").option(\"primaryKeyFields\", 'mainControllerSerialDeviceId').save(delta_table_path)\n",
    "\n",
    "logger.info(f\"Successfully wrote data to delta table in the directory {delta_table_path}\")\n",
    "logger.info('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8599a137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Successful! Logs are saved in output.log\n"
     ]
    }
   ],
   "source": [
    "print(\"Execution Successful! Logs are saved in output.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856362b5-9a05-4abd-8d31-6009bc13ea29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "c2c4311082e7ecaf27cbc2e1450596c6acfefcaffbda68aa46f0091b6f136047"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
